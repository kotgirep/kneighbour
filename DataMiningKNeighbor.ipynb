{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DataMiningKNeighbor.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyONncMqiu8LrWG92smbElCG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kotgirep/kneighbour/blob/main/DataMiningKNeighbor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLXlIE9NktG1"
      },
      "source": [
        "CMPE-255 Data Mining Assignment: \n",
        "\n",
        "Use state of art libraries for Approximate nearest neighbor search for your favorite data set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PG_gDXOk2lV"
      },
      "source": [
        "Import the required libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arEchCT0Kbly"
      },
      "source": [
        "import pickle\n",
        "#import faiss"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCRSu5aHk-Kx"
      },
      "source": [
        "upgrading the pandas libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dj1bRqDWMcDl",
        "outputId": "ff91aeb1-a91c-419d-ed96-95bff3db48b7"
      },
      "source": [
        "pip install --upgrade pandas"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.3.4)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.19.5)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2sjgamuMxhM"
      },
      "source": [
        "import pandas.core.indexes as i"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMxXUBuXlE54"
      },
      "source": [
        "Writing a function to pickle and load dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iGSFhCgJK3gU",
        "outputId": "2db75dc4-a22d-4480-9d0a-33bb467253c8"
      },
      "source": [
        "def pickle_load():\n",
        "    with open('my_pickle_dataset.pickle', 'rb') as f:\n",
        "        pickle_data = pickle.load(f)\n",
        "    return pickle_data\n",
        "\n",
        "pickle_data = pickle_load()\n",
        "pickle_vctr = pickle_data[\"vector\"]\n",
        "names = pickle_data[\"name\"]\n",
        "pickle_data"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'name': array(['Toy Story (1995)', 'GoldenEye (1995)', 'Four Rooms (1995)', ...,\n",
              "        'Sliding Doors (1998)', 'You So Crazy (1994)',\n",
              "        'Scream of Stone (Schrei aus Stein) (1991)'], dtype=object),\n",
              " 'vector': array([[-0.01780608, -0.14265831,  0.10308606, ...,  0.09659795,\n",
              "         -0.17529577, -0.03061521],\n",
              "        [-0.03357764,  0.16418771,  0.21801303, ...,  0.16502103,\n",
              "         -0.09166156,  0.05047869],\n",
              "        [-0.2761452 , -0.01991325, -0.04969981, ...,  0.0258275 ,\n",
              "         -0.08328608, -0.0152858 ],\n",
              "        ...,\n",
              "        [ 0.05142734, -0.01683608, -0.20441587, ...,  0.00045828,\n",
              "          0.14679626,  0.2462584 ],\n",
              "        [ 0.04491899, -0.02819411, -0.09472758, ..., -0.02152078,\n",
              "          0.16223577,  0.19897607],\n",
              "        [ 0.02531924,  0.03099714,  0.06437534, ..., -0.07260127,\n",
              "          0.0467432 ,  0.07893164]], dtype=float32)}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgLAsu79MK9T"
      },
      "source": [
        "Install faiss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_mBrDXBJMB5I",
        "outputId": "f688c364-a338-41b3-991c-c918e5ba39b7"
      },
      "source": [
        "!pip install faiss-gpu"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faiss-gpu\n",
            "  Downloading faiss_gpu-1.7.1.post2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (89.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 89.7 MB 3.1 kB/s \n",
            "\u001b[?25hInstalling collected packages: faiss-gpu\n",
            "Successfully installed faiss-gpu-1.7.1.post2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ShY_cz_4LljX",
        "outputId": "4a77b025-cb36-4057-ff2e-15e8ea2fdc52"
      },
      "source": [
        "import faiss\n",
        "faiss.MatrixStats(pickle_vctr).comments.split(\"\\n\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['analyzing 1682 vectors of size 64',\n",
              " 'no NaN or Infs in data',\n",
              " 'all vectors are distinct',\n",
              " 'range of L2 norms=[0.747558, 1.80436] (0 null vectors)',\n",
              " 'matrix contains no 0s',\n",
              " 'no constant dimensions',\n",
              " 'no dimension has a too large mean',\n",
              " 'stddevs per dimension are in [0.112036 0.158214]',\n",
              " '']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caFSl9p2N26n"
      },
      "source": [
        "Exhaustive Search using Faiss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YyA1HVm_MJ3a"
      },
      "source": [
        "vector_indx = faiss.IndexFlatL2(pickle_vctr.shape[1])\n",
        "vector_indx.add(pickle_vctr)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkWy8n_tOTm3"
      },
      "source": [
        "pickle_search_vctr = pickle_vctr[90:91]\n",
        "distances, indices = vector_indx.search(pickle_search_vctr, 10)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sWGlo4yFOsj5",
        "outputId": "b0b466d4-6171-4cab-a32c-9081089c16fb"
      },
      "source": [
        "print(f\"Approximates same television shows are {names[90]} are:\\n\")\n",
        "print([names[j] for j in indices[0]])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Approximates same television shows are Nightmare Before Christmas, The (1993) are:\n",
            "\n",
            "['Nightmare Before Christmas, The (1993)', 'Heavy Metal (1981)', 'Sirens (1994)', 'Beauty and the Beast (1991)', 'Akira (1988)', 'Fantasia (1940)', 'Benny & Joon (1993)', 'Barbarella (1968)', \"Pete's Dragon (1977)\", 'James and the Giant Peach (1996)']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aw1z2-wUP0YB"
      },
      "source": [
        "Updating clusters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pohdzkLVPBgl"
      },
      "source": [
        "quantizer = faiss.IndexFlatL2(pickle_vctr.shape[1])\n",
        "update_indx = faiss.IndexIVFFlat(quantizer, \n",
        "                           pickle_vctr.shape[1], \n",
        "                           100,             \n",
        "                           8)               \n",
        "update_indx.train(pickle_vctr)\n",
        "update_indx.add(pickle_vctr)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6Jz8jhiQKGh"
      },
      "source": [
        "pickle_search_vctr = pickle_vctr[90:91]\n",
        "distances, indices = vector_indx.search(pickle_search_vctr, 10)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12ADq9c8QZT7",
        "outputId": "e5387c1c-c34f-43e7-8ca6-58025f654fb4"
      },
      "source": [
        "print(f\"Appropriate most same Television shows to watch {names[90]} are:\\n\")\n",
        "print([names[i] for i in indices[0]])"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Appropriate most same Television shows to watch Nightmare Before Christmas, The (1993) are:\n",
            "\n",
            "['Nightmare Before Christmas, The (1993)', 'Heavy Metal (1981)', 'Sirens (1994)', 'Beauty and the Beast (1991)', 'Akira (1988)', 'Fantasia (1940)', 'Benny & Joon (1993)', 'Barbarella (1968)', \"Pete's Dragon (1977)\", 'James and the Giant Peach (1996)']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PClcdULBQqnQ"
      },
      "source": [
        "Product Quantization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aW7EPnJ6QsuS"
      },
      "source": [
        "product_quantizr = faiss.IndexFlatL2(pickle_vctr.shape[1])\n",
        "quantization_indx = faiss.IndexIVFPQ(product_quantizr, \n",
        "                         pickle_vctr.shape[1], \n",
        "                         100,             \n",
        "                         8,                \n",
        "                         8)               \n",
        "quantization_indx.train(pickle_vctr)\n",
        "quantization_indx.add(pickle_vctr)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxCJTTnLRHk3"
      },
      "source": [
        "quant_srch_vctr = pickle_vctr[90:91]\n",
        "distances, indices = quantization_indx.search(quant_srch_vctr, 10)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yazFR28ZRXbP",
        "outputId": "80a4fc5f-0b14-47a4-dded-9f055a5b865c"
      },
      "source": [
        "print(f\"Appropriate most same Television shows to watch {names[90]} are:\\n\")\n",
        "print([names[k] for k in indices[0]])"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Appropriate most same Television shows to watch Nightmare Before Christmas, The (1993) are:\n",
            "\n",
            "['Nightmare Before Christmas, The (1993)', 'Heavy Metal (1981)', 'Pink Floyd - The Wall (1982)', 'Akira (1988)', 'Hamlet (1996)', 'Full Metal Jacket (1987)', 'Supercop (1992)', 'Scream of Stone (Schrei aus Stein) (1991)', 'Scream of Stone (Schrei aus Stein) (1991)', 'Scream of Stone (Schrei aus Stein) (1991)']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqcOXmHuR1jK"
      },
      "source": [
        "Trees and Graphs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjCsq5frSP-9"
      },
      "source": [
        "Install the required libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HMBWPQBmSOsv",
        "outputId": "c0a6e960-c80b-4dd7-9439-0f3720d7395d"
      },
      "source": [
        "!pip install apache_beam\n",
        "!pip install 'scikit_learn~=0.23.0'  # For gaussian_random_matrix.\n",
        "!pip install annoy"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting apache_beam\n",
            "  Downloading apache_beam-2.34.0-cp37-cp37m-manylinux2010_x86_64.whl (9.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.8 MB 5.3 MB/s \n",
            "\u001b[?25hCollecting fastavro<2,>=0.21.4\n",
            "  Downloading fastavro-1.4.7-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3 MB 43.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: httplib2<0.20.0,>=0.8 in /usr/local/lib/python3.7/dist-packages (from apache_beam) (0.17.4)\n",
            "Collecting requests<3.0.0,>=2.24.0\n",
            "  Downloading requests-2.26.0-py2.py3-none-any.whl (62 kB)\n",
            "\u001b[K     |████████████████████████████████| 62 kB 781 kB/s \n",
            "\u001b[?25hCollecting avro-python3!=1.9.2,<1.10.0,>=1.8.1\n",
            "  Downloading avro-python3-1.9.2.1.tar.gz (37 kB)\n",
            "Requirement already satisfied: pytz>=2018.3 in /usr/local/lib/python3.7/dist-packages (from apache_beam) (2018.9)\n",
            "Requirement already satisfied: pymongo<4.0.0,>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from apache_beam) (3.12.1)\n",
            "Requirement already satisfied: protobuf<4,>=3.12.2 in /usr/local/lib/python3.7/dist-packages (from apache_beam) (3.17.3)\n",
            "Requirement already satisfied: oauth2client<5,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from apache_beam) (4.1.3)\n",
            "Requirement already satisfied: pyarrow<6.0.0,>=0.15.1 in /usr/local/lib/python3.7/dist-packages (from apache_beam) (3.0.0)\n",
            "Collecting future<1.0.0,>=0.18.2\n",
            "  Downloading future-0.18.2.tar.gz (829 kB)\n",
            "\u001b[K     |████████████████████████████████| 829 kB 46.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: crcmod<2.0,>=1.7 in /usr/local/lib/python3.7/dist-packages (from apache_beam) (1.7)\n",
            "Requirement already satisfied: typing-extensions<4,>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from apache_beam) (3.10.0.2)\n",
            "Collecting dill<0.3.2,>=0.3.1.1\n",
            "  Downloading dill-0.3.1.1.tar.gz (151 kB)\n",
            "\u001b[K     |████████████████████████████████| 151 kB 52.5 MB/s \n",
            "\u001b[?25hCollecting hdfs<3.0.0,>=2.1.0\n",
            "  Downloading hdfs-2.6.0-py3-none-any.whl (33 kB)\n",
            "Collecting orjson<4.0\n",
            "  Downloading orjson-3.6.4-cp37-cp37m-manylinux_2_24_x86_64.whl (249 kB)\n",
            "\u001b[K     |████████████████████████████████| 249 kB 44.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pydot<2,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from apache_beam) (1.3.0)\n",
            "Requirement already satisfied: grpcio<2,>=1.29.0 in /usr/local/lib/python3.7/dist-packages (from apache_beam) (1.41.1)\n",
            "Requirement already satisfied: numpy<1.21.0,>=1.14.3 in /usr/local/lib/python3.7/dist-packages (from apache_beam) (1.19.5)\n",
            "Requirement already satisfied: python-dateutil<3,>=2.8.0 in /usr/local/lib/python3.7/dist-packages (from apache_beam) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.7/dist-packages (from grpcio<2,>=1.29.0->apache_beam) (1.15.0)\n",
            "Requirement already satisfied: docopt in /usr/local/lib/python3.7/dist-packages (from hdfs<3.0.0,>=2.1.0->apache_beam) (0.6.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.7/dist-packages (from oauth2client<5,>=2.0.1->apache_beam) (0.2.8)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from oauth2client<5,>=2.0.1->apache_beam) (4.7.2)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from oauth2client<5,>=2.0.1->apache_beam) (0.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.1.4 in /usr/local/lib/python3.7/dist-packages (from pydot<2,>=1.2.0->apache_beam) (2.4.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.24.0->apache_beam) (2021.10.8)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.24.0->apache_beam) (1.24.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.24.0->apache_beam) (2.10)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.24.0->apache_beam) (2.0.7)\n",
            "Building wheels for collected packages: avro-python3, dill, future\n",
            "  Building wheel for avro-python3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for avro-python3: filename=avro_python3-1.9.2.1-py3-none-any.whl size=43512 sha256=9dcfbe3b9b4d7e2ab58aa9fe607864a5980e3c0f251dd6d567a80ff7e4697add\n",
            "  Stored in directory: /root/.cache/pip/wheels/bc/49/5f/fdb5b9d85055c478213e0158ac122b596816149a02d82e0ab1\n",
            "  Building wheel for dill (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dill: filename=dill-0.3.1.1-py3-none-any.whl size=78546 sha256=65edc22030b3c8979d852f0ede5ba243ec4c8b186bf60ec1542afc4ae86cd87d\n",
            "  Stored in directory: /root/.cache/pip/wheels/a4/61/fd/c57e374e580aa78a45ed78d5859b3a44436af17e22ca53284f\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=697ebe183eccb5e441d2f9d0682867d243d420d57490fdc1b84834767ec27632\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n",
            "Successfully built avro-python3 dill future\n",
            "Installing collected packages: requests, orjson, hdfs, future, fastavro, dill, avro-python3, apache-beam\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: future\n",
            "    Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "  Attempting uninstall: dill\n",
            "    Found existing installation: dill 0.3.4\n",
            "    Uninstalling dill-0.3.4:\n",
            "      Successfully uninstalled dill-0.3.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "multiprocess 0.70.12.2 requires dill>=0.3.4, but you have dill 0.3.1.1 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas~=1.1.0; python_version >= \"3.0\", but you have pandas 1.3.4 which is incompatible.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.26.0 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed apache-beam-2.34.0 avro-python3-1.9.2.1 dill-0.3.1.1 fastavro-1.4.7 future-0.18.2 hdfs-2.6.0 orjson-3.6.4 requests-2.26.0\n",
            "Collecting scikit_learn~=0.23.0\n",
            "  Downloading scikit_learn-0.23.2-cp37-cp37m-manylinux1_x86_64.whl (6.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.8 MB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit_learn~=0.23.0) (3.0.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit_learn~=0.23.0) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit_learn~=0.23.0) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit_learn~=0.23.0) (1.1.0)\n",
            "Installing collected packages: scikit-learn\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.0.1\n",
            "    Uninstalling scikit-learn-1.0.1:\n",
            "      Successfully uninstalled scikit-learn-1.0.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "imbalanced-learn 0.8.1 requires scikit-learn>=0.24, but you have scikit-learn 0.23.2 which is incompatible.\u001b[0m\n",
            "Successfully installed scikit-learn-0.23.2\n",
            "Collecting annoy\n",
            "  Downloading annoy-1.17.0.tar.gz (646 kB)\n",
            "\u001b[K     |████████████████████████████████| 646 kB 5.1 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: annoy\n",
            "  Building wheel for annoy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for annoy: filename=annoy-1.17.0-cp37-cp37m-linux_x86_64.whl size=391680 sha256=a9c72b2d5bab68597420135f1df3ddf26289a8dec68b32af33584626cd86a8a6\n",
            "  Stored in directory: /root/.cache/pip/wheels/4f/e8/1e/7cc9ebbfa87a3b9f8ba79408d4d31831d67eea918b679a4c07\n",
            "Successfully built annoy\n",
            "Installing collected packages: annoy\n",
            "Successfully installed annoy-1.17.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIuOesA-RiTF"
      },
      "source": [
        "import annoy"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lCf9_KHJSawn",
        "outputId": "74a0b16c-b2e6-4157-eb24-11ea47b22bde"
      },
      "source": [
        "def annoy_load():\n",
        "    with open('my_pickle_dataset.pickle', 'rb') as f:\n",
        "        annoy_data = pickle.load(f)\n",
        "    return annoy_data\n",
        "\n",
        "annoy_data = annoy_load()\n",
        "annoy_data"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'name': array(['Toy Story (1995)', 'GoldenEye (1995)', 'Four Rooms (1995)', ...,\n",
              "        'Sliding Doors (1998)', 'You So Crazy (1994)',\n",
              "        'Scream of Stone (Schrei aus Stein) (1991)'], dtype=object),\n",
              " 'vector': array([[-0.01780608, -0.14265831,  0.10308606, ...,  0.09659795,\n",
              "         -0.17529577, -0.03061521],\n",
              "        [-0.03357764,  0.16418771,  0.21801303, ...,  0.16502103,\n",
              "         -0.09166156,  0.05047869],\n",
              "        [-0.2761452 , -0.01991325, -0.04969981, ...,  0.0258275 ,\n",
              "         -0.08328608, -0.0152858 ],\n",
              "        ...,\n",
              "        [ 0.05142734, -0.01683608, -0.20441587, ...,  0.00045828,\n",
              "          0.14679626,  0.2462584 ],\n",
              "        [ 0.04491899, -0.02819411, -0.09472758, ..., -0.02152078,\n",
              "          0.16223577,  0.19897607],\n",
              "        [ 0.02531924,  0.03099714,  0.06437534, ..., -0.07260127,\n",
              "          0.0467432 ,  0.07893164]], dtype=float32)}"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "os--1N6SS5EE"
      },
      "source": [
        "Annoy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTe6XIudSsqt"
      },
      "source": [
        "class anoy_indx():\n",
        "    def __init__(self, annoy_vectors, total_lbls):\n",
        "        self.dimention = annoy_vectors.shape[1]\n",
        "        self.annoy_vectors = annoy_vectors.astype('float32')\n",
        "        self.total_lbls = total_lbls\n",
        "\n",
        "\n",
        "    def build(self, total_trees=5):\n",
        "        self.index = annoy.AnnoyIndex(self.dimention)\n",
        "        for i, vec in enumerate(self.annoy_vectors):\n",
        "            self.index.add_item(i, vec.tolist())\n",
        "        self.index.build(total_trees)\n",
        "        \n",
        "    def query(self, vector, k=11):\n",
        "        indices = self.index.get_nns_by_vector(vector.tolist(), k)\n",
        "        return [self.total_lbls[l] for l in indices]"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_8wAsnJATnYK",
        "outputId": "497658d6-edef-44a1-c0a5-4ff829b93dad"
      },
      "source": [
        "index = anoy_indx(annoy_data[\"vector\"], annoy_data[\"name\"])\n",
        "index.build()"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: FutureWarning: The default argument for metric will be removed in future version of Annoy. Please pass metric='angular' explicitly.\n",
            "  if __name__ == '__main__':\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HI1fBP6qT-TE",
        "outputId": "f417c332-de51-4bdd-e009-df1e7f460dd1"
      },
      "source": [
        "show_vctr, name_of_the_show = annoy_data['vector'][90], annoy_data['name'][90]\n",
        "same_movies = '\\n* '.join(index.query(show_vctr))\n",
        "print(f\"Appropriate same movies that have to watch {name_of_the_show} are:\\n* {same_movies}\")"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Appropriate same movies that have to watch Nightmare Before Christmas, The (1993) are:\n",
            "* Nightmare Before Christmas, The (1993)\n",
            "* Aladdin (1992)\n",
            "* Blade Runner (1982)\n",
            "* Aliens (1986)\n",
            "* Pink Floyd - The Wall (1982)\n",
            "* Brazil (1985)\n",
            "* Wrong Trousers, The (1993)\n",
            "* Alien (1979)\n",
            "* Return of the Jedi (1983)\n",
            "* E.T. the Extra-Terrestrial (1982)\n",
            "* Grand Day Out, A (1992)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeAK252GUmtR"
      },
      "source": [
        "Graphs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gPITGBXNUpXo",
        "outputId": "97e94c1e-8394-4668-cf33-0aa40cdef29c"
      },
      "source": [
        "!pip install nmslib"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nmslib\n",
            "  Downloading nmslib-2.1.1-cp37-cp37m-manylinux2010_x86_64.whl (13.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 13.5 MB 50 kB/s \n",
            "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from nmslib) (5.4.8)\n",
            "Requirement already satisfied: numpy>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from nmslib) (1.19.5)\n",
            "Collecting pybind11<2.6.2\n",
            "  Downloading pybind11-2.6.1-py2.py3-none-any.whl (188 kB)\n",
            "\u001b[K     |████████████████████████████████| 188 kB 33.2 MB/s \n",
            "\u001b[?25hInstalling collected packages: pybind11, nmslib\n",
            "Successfully installed nmslib-2.1.1 pybind11-2.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G205xCxjUc76"
      },
      "source": [
        "import nmslib"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zz31yr1wiEmc",
        "outputId": "91048900-ebf7-426b-f533-9da863f599d9"
      },
      "source": [
        "def graph_load_data():\n",
        "    with open('my_pickle_dataset.pickle', 'rb') as f:\n",
        "        graph_data = pickle.load(f)\n",
        "    return graph_data\n",
        "\n",
        "graph_data = graph_load_data()\n",
        "graph_data"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'name': array(['Toy Story (1995)', 'GoldenEye (1995)', 'Four Rooms (1995)', ...,\n",
              "        'Sliding Doors (1998)', 'You So Crazy (1994)',\n",
              "        'Scream of Stone (Schrei aus Stein) (1991)'], dtype=object),\n",
              " 'vector': array([[-0.01780608, -0.14265831,  0.10308606, ...,  0.09659795,\n",
              "         -0.17529577, -0.03061521],\n",
              "        [-0.03357764,  0.16418771,  0.21801303, ...,  0.16502103,\n",
              "         -0.09166156,  0.05047869],\n",
              "        [-0.2761452 , -0.01991325, -0.04969981, ...,  0.0258275 ,\n",
              "         -0.08328608, -0.0152858 ],\n",
              "        ...,\n",
              "        [ 0.05142734, -0.01683608, -0.20441587, ...,  0.00045828,\n",
              "          0.14679626,  0.2462584 ],\n",
              "        [ 0.04491899, -0.02819411, -0.09472758, ..., -0.02152078,\n",
              "          0.16223577,  0.19897607],\n",
              "        [ 0.02531924,  0.03099714,  0.06437534, ..., -0.07260127,\n",
              "          0.0467432 ,  0.07893164]], dtype=float32)}"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EDCFFgIUwAr"
      },
      "source": [
        "class hnsw_indx():\n",
        "    def __init__(self, vectors, labels):\n",
        "        self.dimention = vectors.shape[1]\n",
        "        self.vectors = vectors.astype('float32')\n",
        "        self.labels = labels\n",
        "\n",
        "    def build(self):\n",
        "        self.index = nmslib.init(method='hnsw', space='cosinesimil')\n",
        "        self.index.addDataPointBatch(self.vectors)\n",
        "        self.index.createIndex({'post': 2})\n",
        "        \n",
        "    def query(self, vector, k=10):\n",
        "        indices = self.index.knnQuery(vector, k=k)\n",
        "        return [self.labels[z] for z in indices[0]]"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-D54ESeIidXa"
      },
      "source": [
        "graph_indx = hnsw_indx(graph_data[\"vector\"], graph_data[\"name\"])\n",
        "graph_indx.build()"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ccGQLUpGipmR",
        "outputId": "813fd87d-56d0-45af-e604-5d7b0fd35eaa"
      },
      "source": [
        "graph_vectr, show_name = graph_data['vector'][90], graph_data['name'][90]\n",
        "same_shows = '\\n* '.join(graph_indx.query(graph_vectr))\n",
        "print(f\"Appropriate similar tv shows {show_name} are:\\n* {same_shows}\")"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Appropriate similar tv shows Nightmare Before Christmas, The (1993) are:\n",
            "* Nightmare Before Christmas, The (1993)\n",
            "* Beauty and the Beast (1991)\n",
            "* Fantasia (1940)\n",
            "* Heavy Metal (1981)\n",
            "* Aladdin (1992)\n",
            "* Snow White and the Seven Dwarfs (1937)\n",
            "* Batman (1989)\n",
            "* James and the Giant Peach (1996)\n",
            "* Blade Runner (1982)\n",
            "* Aliens (1986)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTHXOLmujLJ4"
      },
      "source": [
        "Locality Sensitive Hashing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4fN9jjNjNoC"
      },
      "source": [
        "class sensitivehashing():\n",
        "    def __init__(self, lsh_vctrs, lsh_lbls):\n",
        "        self.dimention = lsh_vctrs.shape[1]\n",
        "        self.lsh_vctrs = lsh_vctrs.astype('float32')\n",
        "        self.lsh_lbls = lsh_lbls\n",
        "\n",
        "\n",
        "    def build(self, number_of_partition=8, search_in_x_partitions=2, subvector_size=8):\n",
        "        lsh_quant = faiss.IndexFlatL2(self.dimention)\n",
        "        self.graph_indx = faiss.IndexIVFPQ(lsh_quant, self.dimention, number_of_partition, search_in_x_partitions, subvector_size)\n",
        "        self.graph_indx.train(self.lsh_vctrs)\n",
        "        self.graph_indx.add(self.lsh_vctrs)\n",
        "        \n",
        "    def query(self, lsh_vctrs, k=10):\n",
        "        distances, indices = self.graph_indx.search(lsh_vctrs, k) \n",
        "        return [self.lsh_lbls[a] for a in indices[0]]"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "msp1g5VWkFqn"
      },
      "source": [
        "graph_indx = sensitivehashing(graph_data[\"vector\"], graph_data[\"name\"])\n",
        "graph_indx.build()"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qcq53eQvkR8r",
        "outputId": "dff2f814-4a42-45a8-83b5-16929d5ee0cf"
      },
      "source": [
        "graph_vector, name_of_the_show = graph_data['vector'][90:91], graph_data['name'][90]\n",
        "Movie_exact_same_names = '\\n* '.join(graph_indx.query(graph_vector))\n",
        "print(f\"The most similar movies to {name_of_the_show} are:\\n* {Movie_exact_same_names}\")"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The most similar movies to Nightmare Before Christmas, The (1993) are:\n",
            "* Nightmare Before Christmas, The (1993)\n",
            "* Fantasia (1940)\n",
            "* Brazil (1985)\n",
            "* Monty Python's Life of Brian (1979)\n",
            "* This Is Spinal Tap (1984)\n",
            "* Hunt for Red October, The (1990)\n",
            "* Sneakers (1992)\n",
            "* Lion King, The (1994)\n",
            "* Clockwork Orange, A (1971)\n",
            "* Full Metal Jacket (1987)\n"
          ]
        }
      ]
    }
  ]
}